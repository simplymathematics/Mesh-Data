---
title: "Project 2"
author: "simplymathematics"
date: "October 7, 2018"
output:
  html_document: default
  latex_engine: xelatex

---
Instead of using three different databases, I wanted to build a reference database for the data I collected previously. The first step, as always, is to get your environment ready.

```{r}
library(curl, quietly = TRUE)
library(XML, quietly= TRUE)
library(stringr, quietly= TRUE)
suppressPackageStartupMessages(library(tidyverse, quietly = TRUE))

```
# Internet Exchange Points
I wanted an update-able dataset of Internet Exchange points around the world. Each NIC (regional IP/TCP benevolent overlords) has datasets of their own, but I'd have to parse each individually. Wikipedia seems to keep an accurate enough dataset. 

Then, I used curl to import the data and the XML library to parse it as a tree
```{r}

data.file <- curl_download("https://en.wikipedia.org/wiki/List_of_Internet_exchange_points_by_size", "IXPs/ixps.html")

raw.data <- readHTMLTable(data.file)

```
Then, I had set the first row of the data frame as the column names.
```{r}
data <- data.frame(raw.data[])
colnames(data) <- as.character(unlist(data[1,]))
data = data[-1, ]
head(data)
```
Finally, I wanted to spread the data so that each city had its own listing with corresponding provider information. First I had to separate the cities and treat them as independent variables.
```{r}

data <- mutate(data, City = strsplit(as.character(City), ",")) %>% 
    unnest(City)

```

I then had to clean up the cities and figure out how many connections they each had.
```{r}
cities <- unique(trimws(data$City)) #cleaning
connections_per_city <- data.frame()
for (city in cities){ #counting
connections <- dim(subset(data, City == city))[1]
new.row <- cbind(city, connections)
connections_per_city <- rbind(new.row, connections_per_city)
}
one <- sum(connections_per_city$connections == 1) #binning
two <- sum(connections_per_city$connections == 2)
three <- sum(connections_per_city$connections == 3)
four <- sum(connections_per_city$connections == 4)
five <- sum(connections_per_city$connections == 5)
distribution <- cbind(one, two, three, four, five)
barplot(distribution, legend.text = "Number of Connections within City")
write.csv(data, file = "IXPs.csv")
```

This data is stil messy, in particular because the city data isn't standardized. For example, the unique() fucntion does not understand the similarity between "NYC" and " New York City". Manual work will have to be done to collapse all of these cities. Additionally, I do not know whether Seacaucus NJ should count as NYC for the purposes of this project since it serves the same metropolitan area and fiber optic signals travel nearly the spped of light. However, despite this limited issue, we can see that the vast majority of cities in the world have only a single high-level connection to *the* internet, and that that privilege is only granted to a few cities. 


# Fiber to the Home
I also wanted a list of cities and municipalities that hand locally-controlled infrastructure. For that, I scraped the Muninetworks website.
```{r}
data.file2 <- curl_download("https://muninetworks.org/content/municipal-ftth-networks", "FTTH/ftth.html")
data.file2
```
Next, I read the file in, line by line.
```{r}
lines <- readLines(data.file2)
head(lines)
```
Then, I used a text editor to find the first line in my dataset. From there, I reconstructed the html table using regex.
```{r}
first.chunk <- which(grepl("<p><strong>", lines))
#lines[first.chunk]
networks <- c(str_extract(lines[first.chunk], "(?<=<strong>)(.*\n?)(?=</strong>)"))
communities <- c(str_extract(lines[first.chunk+1], "(?<=Served: )(.*)(?=</em>)" ))
years <- c(str_extract(lines[first.chunk+3], "(?<=Year: )(.*)(?=</li>)" ))
populations <- c(str_extract(lines[first.chunk+4], "(?<=Population: )(.*)(?=</li>)" ))
costs <- c(str_extract(lines[first.chunk+5], "(?<=Cost: )(.*)(?=</li>)"))
funding.methods <- c(str_extract(lines[first.chunk+6], "(?<=Method: )(.*)(?=</li>)" ))
governances <- c(str_extract(lines[first.chunk+7], "(?<=Governance: )(.*)(?=</li>)" ))
services <- c(str_extract(lines[first.chunk+8], "(?<=Services: )(.*)(?=</li>)" ))
speeds <- c(str_extract(lines[first.chunk+9], "(?<=Speed: )(.*)(?=</li>)" ))
costs
```
Then I bound all the variables into a dataframe and separated the services.
```{r}
data <- data.frame(cbind(networks, communities, years, populations, costs, funding.methods, governances, services, speeds))
data <- separate(data, services, into = c("Service1", "Service2", "Service3"), sep = ',')
head(data,10)

```
Then I wrote it all to a csv
```{r}
write.csv(data, file = "ftth.csv")
```
For my analysis, I wanted to see the cost/per person of building a fiber network. So I cleaned up the data by pulling out the dollar figures and removing the NAs. 
```{r}
communities <- data.frame(communities)
data <- data.frame(data)

#Funds (in millions)
funds <- str_extract(data$costs, "[0-9]{1,4}")
funds <- funds[!is.na(funds)]
funds = as.double(funds)
mean_f = mean(funds)
#Population #(in thousands)
pop <- str_extract(data$populations, "[0-9]{1,7}")
pop <- pop[!is.na(pop)]
pop <- as.double(pop)
mean_p <- mean(pop)

#Wrapping it Up
dollar.per.user = mean_f /mean_p
dollar.per.user
```
The average community network costs about 1,0049 per person! Below, we can find the maximum lifespan of one of these networks. 

```{r}
year <- c(str_extract(data$year, "[0-9]{4}"))
year <- year[!is.na(year)]
year <- as.integer(year)
print("Minimum:")
min(year)
print("Summary:")
summary(year)
data[22,]
```
The oldest network on the list is the Marshall, Missouri Municipal Utilities Corporation. With a coaxial network to each home (reflected in the 90/45 transfer rate), this network not only surpasses many places in the world, but it is self-managed by the city. It is not as fast as a brand-new fiber network, but easily serves Netflix to a household at peak times. If we amortize these costs over 16 years, we can find our cost per person per year. Even if we look at the average lifespan, we see that the cost is only $116/user/year. 
```{r}
dollar.per.user.per.year = dollar.per.user/(2018 - 2002)
dollar.per.user.per.year * 1000 # converting 10^6 dollars/10^3 people

less.optimistic = dollar.per.user/(2018-2009) *1000
less.optimistic

```
With reasonable optimism, we see that municipal a network costs about $65/user/year. The only other concern would be the missing data from the the cost and population datasets. These may be outliers in actuality, which could dramatically shift this average.

# Mac Addresses
Finally, I wanted to be able to track the types of devices so that I can do more deep network intelligence. First, I have to load the dataset from the IEEE (available as a .txt).

```{r}
data.file3 <- curl_download("http://standards-oui.ieee.org/oui.txt", "MACs/IEEE-MACs.txt")
data.file3
```
So, I parsed long data, converting it into wide data. Because the delimiter isn't constant, tidyr doesn't help much herre.

```{r}
lines <- readLines(data.file3)
#head(lines)

first.chunk <- which(grepl("[0-9A-F]{2}-[0-9A-F]{2}-[0-9A-F]{2}", lines))
lines1 = lines[first.chunk]
lines2 = str_extract_all(lines[first.chunk+2], "(?<=\\t\\t\\t\\t)(.*)")
lines3 = str_extract_all(lines[first.chunk+3], "(?<=\\t\\t\\t\\t)(.*)")
lines4 = lines[first.chunk+4]
```

Then I had to parse each of these lines and extract their data points. I assigned each one of these to a vector corresponding to to row. Then, I bound all the data together and wrote it to a csv.

```{r}
MACs <- c(str_extract(lines1, "[0-9A-F]{2}-[0-9A-F]{2}-[0-9A-F]{2}"))
Manufacturers <- c(str_extract(lines1, "(?<=\\t\\t)(.*)"))
Addresses <- c(lines2)
Zips <- c(str_extract(lines3, "[0-9]{5}"))
Region <- c(str_extract(lines3, "([^[0-9]{5}]+)"))
Country <- c(str_extract(lines4, "[:alpha:]{2}"))
data <- (cbind(MACs, Manufacturers, Addresses, Zips, Region, Country))
head(data)
write.csv(data, file = "IEEE-MACs.csv")

```
I'm repeating the same geographic analysis as above, but this time looking at countries where hardware is produced. 
```{r}
per_country = data.frame()
data <- data.frame(Country)
country.list <- unique(trimws(data$Country))
for (country in country.list){
 number <- dim(subset(data, Country == country))[[1]]
 new.row <- cbind(country, number)
 per_country <- rbind(per_country, new.row)
}
per_country <- data.frame(per_country)
arrange(per_country, number)
```
We can see from this data that the US has 3 times as many networked devices manufacturers than China, despite other assumptions. Additionally, both Germany and Taiwan have significant investments in this field. Below are some dependencies for a map. 

```{r}
#install.packages("countrycode")
#install.packages("rworldmap")
suppressMessages(library(countrycode))
suppressMessages(library(rworldmap))
```

```{r}
full.name <- countrycode(per_country$country, "iso2c", "country.name", nomatch = NULL )
per_country <- cbind(per_country, full.name)

ieee.manufacturers.per.country <- joinCountryData2Map(per_country, joinCode = "ISO2", nameJoinColumn = "country")
mapCountryData(ieee.manufacturers.per.country, nameColumnToPlot = "number", mapTitle ="Distinct IEEE Manufacturers per Country", addLegend =FALSE, colourPalette =  "heat", missingCountryCol = "black", oceanCol = "black" )

```

This map corroborates the data from section 1. Both indicate that only a few places in the world are privileged with Internet access despite the relative affordabliity for instrastructure. 
