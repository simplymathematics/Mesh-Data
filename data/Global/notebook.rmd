---
title: "Project 2"
author: "simplymathematics"
date: "October 7, 2018"
output: html_document
---
Instead of using three different databases, I wanted to build a reference database for the data I collected previously. The first step, as always, is to get your environment ready.

```{r}
library(curl)
library(XML)
library(stringr)
library(tidyverse)

```
## Internet Exchange Points
I wanted an update-able dataset of Internet Exchange points around the world. Each NIC (regional IP/TCP benevolent overlords) has datasets of their own, but I'd have to parse each individually. Wikipedia seems to keep an accurate enough dataset. 

Then, I used curl to import the data and the XML library to parse it as a tree
```{r}

data.file <- curl_download("https://en.wikipedia.org/wiki/List_of_Internet_exchange_points_by_size", "IXPs/ixps.html")

raw.data <- readHTMLTable(data.file)

```
Then, I had set the first row of the data frame as the column names.
```{r}
data <- data.frame(raw.data[])
colnames(data) <- as.character(unlist(data[1,]))
data = data[-1, ]
data
```
Finally, I wanted to spread the data so that each city had its own listing with corresponding provider information. First I had to separate the cities and treat them as independent variables.
```{r}

data <- mutate(data, City = strsplit(as.character(City), ",")) %>% 
    unnest(City)

```

I then had to cleam up the cities and figure out how many connections they each had.
```{r}
cities <- unique(trimws(data$City))
connections_per_city <- data.frame()
for (city in cities){
connections <- dim(subset(data, City == city))[1]
new.row <- cbind(city, connections)
connections_per_city <- rbind(new.row, connections_per_city)
}
ones <- sum(connections_per_city$connections == 1)
twos <- sum(connections_per_city$connections == 2)
threes <- sum(connections_per_city$connections == 3)
fours <- sum(connections_per_city$connections == 4)
distribution <- cbind(ones, twos, threes, fours)
barplot(distribution)
write.csv(data, file = "IXPs.csv")
```
### Conclusion
This data is stil messy, in particular because the city data isn't standardized. The unique() does not understand the similarity between "NYC" and " New York City". Manual work will have to be done to collapse all of these cities. Additionally, I do not know whether Seacaucus NJ should count as NYC for the purposes of this project since it serves the city. In the vast majority of other cases, this is not an issue. Despite this, we can see that the vast majority of cities in the world have only a single high-level connection to *the* internet. 

## Fiber to the Home
I also wanted a list of cities and municipalities that hand locally-controlled infrastructure. For that, I scraped the Muninetworks website.
```{r}
data.file2 <- curl_download("https://muninetworks.org/content/municipal-ftth-networks", "FTTH/ftth.html")
data.file2
```
Next, I read the file in, line by line.
```{r}
lines <- readLines(data.file2)
head(lines)
```
Then, I used a text editor to find the first line in my dataset. From there, I reconstructed the html table using regex.
```{r}
first.chunk <- which(grepl("<p><strong>", lines))
#lines[first.chunk]
networks <- c(str_extract(lines[first.chunk], "(?<=<strong>)(.*\n?)(?=</strong>)"))
communities <- c(str_extract(lines[first.chunk+1], "(?<=Served: )(.*)(?=</em>)" ))
years <- c(str_extract(lines[first.chunk+3], "(?<=Year: )(.*)(?=</li>)" ))
populations <- c(str_extract(lines[first.chunk+4], "(?<=Population: )(.*)(?=</li>)" ))
costs <- c(str_extract(lines[first.chunk+5], "(?<=Cost: )(.*)(?=</li>)"))
funding.methods <- c(str_extract(lines[first.chunk+6], "(?<=Method: )(.*)(?=</li>)" ))
governances <- c(str_extract(lines[first.chunk+7], "(?<=Governance: )(.*)(?=</li>)" ))
services <- c(str_extract(lines[first.chunk+8], "(?<=Services: )(.*)(?=</li>)" ))
speeds <- c(str_extract(lines[first.chunk+9], "(?<=Speed: )(.*)(?=</li>)" ))
```
Then I bound all the variables into a dataframe and wrote them to a csv.
```{r}
data <- (cbind(networks, communities, years, populations, costs, funding.methods, governances, services, speeds))
head(data)
write.csv(data, file = "IXPs.csv")
```
For my analysis, I wanted to see the cost/per person of building a fiber network. So I cleaned up the data by pulling out the dollar figures and removing the NAs. 
```{r}
communities <- data.frame(communities)
data <- data.frame(data)

#Funds (in millions)
funds <- str_extract(data$costs, "[0-9]{1,4}")
funds <- funds[!is.na(funds)]
funds = as.double(funds)
mean_f = mean(funds)

#Population
pop <- str_extract(data$populations, "[0-9]{1,7}")
pop <- pop[!is.na(pop)]
pop <- as.double(pop)
mean_p <- mean(pop)

#Wrapping it Up
mean_f /mean_p
```
The average community network costs about 1,000,000 per person! Clearly something is wrong here! This might explain our results from section 1.

## Mac Addresses
Finally, I wanted to be able to track the types of devices so that I can do more deep network intelligence. First, I have to load the dataset from the IEEE (available as a .txt).

```{r}
data.file3 <- curl_download("http://standards-oui.ieee.org/oui.txt", "MACs/IEEE-MACs.txt")
data.file3
```
So, I parsed long data, converting it into wide data. Because the delimiter isn't constant, tidyr doesn't help much herre.

```{r}
lines <- readLines(data.file3)
#head(lines)

first.chunk <- which(grepl("[0-9A-F]{2}-[0-9A-F]{2}-[0-9A-F]{2}", lines))
lines1 = lines[first.chunk]
lines2 = str_extract_all(lines[first.chunk+2], "(?<=\\t\\t\\t\\t)(.*)")
lines3 = str_extract_all(lines[first.chunk+3], "(?<=\\t\\t\\t\\t)(.*)")
lines4 = lines[first.chunk+4]
```

Then I had to parse each of these lines and extract their data points. I assigned each one of these to a vector corresponding to to row. Then, I bound all the data together and wrote it to a csv.

```{r}
MACs <- c(str_extract(lines1, "[0-9A-F]{2}-[0-9A-F]{2}-[0-9A-F]{2}"))
Manufacturers <- c(str_extract(lines1, "(?<=\\t\\t)(.*)"))
Addresses <- c(lines2)
Zips <- c(str_extract(lines3, "[0-9]{5}"))
Region <- c(str_extract(lines3, "([^[0-9]{5}]+)"))
Country <- c(str_extract(lines4, "[:alpha:]{2}"))
data <- (cbind(MACs, Manufacturers, Addresses, Zips, Region, Country))
head(data)
write.csv(data, file = "IEEE-MACs.csv")

```
I'm repeating the same geographic analysis as above, but this time looking at zip codes. 
```{r}
per_country = data.frame()
data <- data.frame(Country)
country.list <- unique(trimws(data$Country))
for (country in country.list){
 number <- dim(subset(data, Country == country))[[1]]
 new.row <- cbind(country, number)
 per_country <- rbind(per_country, new.row)
}
per_country <- data.frame(per_country)
arrange(per_country, number)
```
We can see from this data that the US has 3 times as many networked devices manufacturers than China, despite other assumptions. Additionally, both Germany and Taiwan have significant investments in this field.

