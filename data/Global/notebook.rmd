---
title: "Project 2"
author: "simplymathematics"
date: "October 7, 2018"
output: html_document
---
Instead of using three different databases, I wanted to build a reference database for the data I collected previously. The first step, as always, is to get your environment ready.

```{r}
library(curl)
library(XML)
library(stringr)
library(tidyverse)

```
## Internet Exchange Points
I wanted an update-able dataset of Internet Exchange points around the world. Each NIC (regional IP/TCP benevolent overlords) has datasets of their own, but I'd have to parse each individually. Wikipedia seems to keep an accurate enough dataset. 

Then, I used curl to import the data and the XML library to parse it as a tree
```{r}

data.file <- curl_download("https://en.wikipedia.org/wiki/List_of_Internet_exchange_points_by_size", "IXPs/ixps.html")

raw.data <- readHTMLTable(data.file)

```
Then, I had set the first row of the data frame as the column names.
```{r}
data <- data.frame(raw.data[])
colnames(data) <- as.character(unlist(data[1,]))
data = data[-1, ]
data
```
Finally, I wanted to spread the data so that each city had its own listing with corresponding provider information. First I had to separate the cities and treat them as independent variables.
```{r}

data <- mutate(data, City = strsplit(as.character(City), ",")) %>% 
    unnest(City)

```

I then had to cleam up the cities and figure out how many connections they each had.
```{r}
cities <- unique(trimws(data$City))
connections_per_city <- data.frame()
for (city in cities){
connections <- dim(subset(data, City == city))[1]
new.row <- cbind(city, connections)
connections_per_city <- rbind(new.row, connections_per_city)
}
ones <- sum(connections_per_city$connections == 1)
twos <- sum(connections_per_city$connections == 2)
threes <- sum(connections_per_city$connections == 3)
fours <- sum(connections_per_city$connections == 4)
distribution <- cbind(ones, twos, threes, fours)
barplot(distribution)
write.csv(data, file = "/IXPs/IXPs.csv")
```
### Conclusion
This data is stil messy, in particular because the city data isn't standardized. The unique() does not understand the similarity between "NYC" and " New York City". Manual work will have to be done to collapse all of these cities. Additionally, I do not know whether Seacaucus NJ should count as NYC for the purposes of this project since it serves the city. In the vast majority of other cases, this is not an issue. Despite this, we can see that the vast majority of cities in the world have only a single high-level connection to *the* internet. 

## Fiber to the Home
I also wanted a list of cities and municipalities that hand locally-controlled infrastructure. For that, I scraped the Muninetworks website.
```{r}
data.file2 <- curl_download("https://muninetworks.org/content/municipal-ftth-networks", "FTTH/ftth.html")
data.file2
```

```{r}
lines <- readLines(data.file2)
head(lines)
```
```{r}
first.chunk <- which(grepl("<p><strong>", lines))
#lines[first.chunk]
networks <- str_extract(lines[first.chunk], "(?<=<strong>)(.*\n?)(?=</strong>)")
communities <- str_extract(lines[first.chunk+1], "(?<=Served: )(.*)(?=</em>)" )
years <- str_extract(lines[first.chunk+3], "(?<=Year: )(.*)(?=</li>)" )
populations <- str_extract(lines[first.chunk+4], "(?<=Population: )(.*)(?=</li>)" )
costs <- str_extract(lines[first.chunk+5], "(?<=Cost: )(.*)(?=</li>)" )
funding.methods <- str_extract(lines[first.chunk+6], "(?<=Method: )(.*)(?=</li>)" )
governances <- str_extract(lines[first.chunk+7], "(?<=Governance: )(.*)(?=</li>)" )
services <- str_extract(lines[first.chunk+8], "(?<=Services: )(.*)(?=</li>)" )
speeds <- str_extract(lines[first.chunk+9], "(?<=Speed: )(.*)(?=</li>)" )

data <- data.frame(cbind(networks, communities, years, populations, costs, funding.methods, governances, services, speeds))
data
```
## Mac Addresses
Finally, I wanted to be able to track the types of devices so that I can do more deep network intelligence. First, I have to load the dataset from the IEEE (available as a .txt).

```{r}
data.file3 <- curl_download("http://standards-oui.ieee.org/oui.txt", "MACs/IEEE-MACs.txt")
data.file3
```
So, I parsed long data, converting it into wide data. Because the delimiter isn't constant, tidyr doesn't help much herre.

```{r}
lines <- readLines(data.file3)
#head(lines)

first.chunk <- which(grepl("[0-9A-F]{2}-[0-9A-F]{2}-[0-9A-F]{2}", lines))
lines1 = lines[first.chunk]
lines2 = str_extract_all(lines[first.chunk+2], "(?<=\\t\\t\\t\\t)(.*)")
lines3 = str_extract_all(lines[first.chunk+3], "(?<=\\t\\t\\t\\t)(.*)")
lines4 = lines[first.chunk+4]
```

Then I had to parse each of these lines and extract their data points. I assigned each one of these to a vector corresponding to to row. Then, I bound all the data together and wrote it to a csv.

```{r}
MACs <- c(str_extract(lines1, "[0-9A-F]{2}-[0-9A-F]{2}-[0-9A-F]{2}"))
Manufacturers <- c(str_extract(lines1, "(?<=\\t\\t)(.*)"))
Addresses <- c(lines2)
Zips <- c(str_extract(lines3, "[0-9]{5}"))
Region <- c(str_extract(lines3, "([^[0-9]{5}]+)"))
Country <- c(str_extract(lines4, "[:alpha:]{2}"))
data <- cbind(MACs, Manufacturers, Addresses, Zips, Region, Country)
write.csv(data, file = "MACs/IEEE-MACs.csv")
```

